{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tH154dXq2df"
   },
   "source": [
    "# ML - Problem Set 2: Predicting Poverty\n",
    "\n",
    "El objetivo principal es construir un modelo predictivo de la pobreza de los hogares. Nótese que un hogar se clasifica como\n",
    "\n",
    "\\begin{equation}\n",
    "    Poor = I \\left( Inc < Pl \\right)\n",
    "\\end{equation}\n",
    "\n",
    "donde $I$ es una función indicadora que toma uno si el ingreso familiar está por debajo de una determinada línea de pobreza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahg9cilhs_nk"
   },
   "source": [
    "## Paso 1. Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57HpxY2Iq0cO"
   },
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ea5SW6xrrDY8"
   },
   "outputs": [],
   "source": [
    "# Importar datos\n",
    "\n",
    "test_hogares = pd.read_csv('test_hogares.csv')\n",
    "train_hogares = pd.read_csv('train_hogares.csv')\n",
    "test_personas = pd.read_csv('test_personas.csv')\n",
    "train_personas = pd.read_csv('train_personas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irnH00CtcNex",
    "outputId": "0c29cd67-f2a3-466a-c4a3-bce846ac21d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Limpieza de datos\n",
    "\n",
    "train_personas['P6210s1'] = train_personas['P6210s1'].clip(lower=0, upper=15) #eliminar valores extremos\n",
    "test_personas['P6210s1'] = test_personas['P6210s1'].clip(lower=0, upper=15) #eliminar valores extremos\n",
    "\n",
    "train_personas['P7510s5'] = train_personas['P7510s5'].clip(lower=1, upper=2) #eliminar valores extremos\n",
    "test_personas['P7510s5'] = test_personas['P7510s5'].clip(lower=1, upper=2) #eliminar valores extremos\n",
    "\n",
    "train_personas['Des'] = train_personas['Des'].fillna(0) #reemplazar nan por 0\n",
    "train_personas['Ina'] = train_personas['Ina'].fillna(0) #reemplazar nan por 0\n",
    "test_personas['Des'] = test_personas['Des'].fillna(0) #reemplazar nan por 0\n",
    "test_personas['Ina'] = test_personas['Ina'].fillna(0) #reemplazar nan por 0\n",
    "\n",
    "valores_unicos = sorted(train_hogares['Pobre'].unique())\n",
    "print(valores_unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHLWslcIG3x6",
    "outputId": "28a42805-02c0-4667-8047-1f79093c44e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores faltantes después de imputación:\n",
      "test_hogares: 0\n",
      "train_hogares: 0\n",
      "test_personas: 0\n",
      "train_personas: 0\n"
     ]
    }
   ],
   "source": [
    "# Imputar valores faltantes\n",
    "\n",
    "imputador_moda = SimpleImputer(strategy='most_frequent')  # Para variables categóricas\n",
    "imputador_mediana = SimpleImputer(strategy='median')  # Para variables numéricas\n",
    "\n",
    "# Función para imputar datos faltantes con SimpleImputer\n",
    "def imputar_datos_simple(df):\n",
    "    # Separar columnas categóricas y numéricas\n",
    "    columnas_categoricas = df.select_dtypes(exclude=['number']).columns\n",
    "    columnas_numericas = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Imputar categóricas con la moda\n",
    "    if not columnas_categoricas.empty:\n",
    "        df[columnas_categoricas] = imputador_moda.fit_transform(df[columnas_categoricas])\n",
    "\n",
    "    # Imputar numéricas con la mediana\n",
    "    if not columnas_numericas.empty:\n",
    "        df[columnas_numericas] = imputador_mediana.fit_transform(df[columnas_numericas])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Cargar los datos desde archivos CSV (asegúrate de que las rutas sean correctas)\n",
    "test_hogares = pd.read_csv('test_hogares.csv')\n",
    "train_hogares = pd.read_csv('train_hogares.csv')\n",
    "test_personas = pd.read_csv('test_personas.csv')\n",
    "train_personas = pd.read_csv('train_personas.csv')\n",
    "\n",
    "# Aplicar la imputación simple a cada DataFrame\n",
    "test_hogares = imputar_datos_simple(test_hogares)\n",
    "train_hogares = imputar_datos_simple(train_hogares)\n",
    "test_personas = imputar_datos_simple(test_personas)\n",
    "train_personas = imputar_datos_simple(train_personas)\n",
    "\n",
    "# Verificar valores faltantes después de imputación\n",
    "print(\"Valores faltantes después de imputación:\")\n",
    "print(\"test_hogares:\", test_hogares.isnull().sum().sum())\n",
    "print(\"train_hogares:\", train_hogares.isnull().sum().sum())\n",
    "print(\"test_personas:\", test_personas.isnull().sum().sum())\n",
    "print(\"train_personas:\", train_personas.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzEAphkuP6Sw",
    "outputId": "2268d5ef-2e5b-465a-d023-4135913434be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Clase', 'Dominio', 'P5000', 'P5010', 'P5090', 'P5100', 'P5130', 'P5140', 'Nper', 'Npersug', 'Li', 'Lp', 'Fex_c', 'Depto', 'Fex_dpto']\n",
      "['id', 'Clase', 'Dominio', 'P5000', 'P5010', 'P5090', 'P5100', 'P5130', 'P5140', 'Nper', 'Npersug', 'Ingtotug', 'Ingtotugarr', 'Ingpcug', 'Li', 'Lp', 'Pobre', 'Indigente', 'Npobres', 'Nindigentes', 'Fex_c', 'Depto', 'Fex_dpto']\n",
      "['id', 'Orden', 'Clase', 'Dominio', 'P6020', 'P6040', 'P6050', 'P6090', 'P6100', 'P6210', 'P6210s1', 'P6240', 'Oficio', 'P6426', 'P6430', 'P6510', 'P6545', 'P6580', 'P6585s1', 'P6585s2', 'P6585s3', 'P6585s4', 'P6590', 'P6600', 'P6610', 'P6620', 'P6630s1', 'P6630s2', 'P6630s3', 'P6630s4', 'P6630s6', 'P6800', 'P6870', 'P6920', 'P7040', 'P7045', 'P7050', 'P7090', 'P7110', 'P7120', 'P7150', 'P7160', 'P7310', 'P7350', 'P7422', 'P7472', 'P7495', 'P7500s2', 'P7500s3', 'P7505', 'P7510s1', 'P7510s2', 'P7510s3', 'P7510s5', 'P7510s6', 'P7510s7', 'Pet', 'Oc', 'Des', 'Ina', 'Fex_c', 'Depto', 'Fex_dpto']\n",
      "['id', 'Orden', 'Clase', 'Dominio', 'Estrato1', 'P6020', 'P6040', 'P6050', 'P6090', 'P6100', 'P6210', 'P6210s1', 'P6240', 'Oficio', 'P6426', 'P6430', 'P6500', 'P6510', 'P6510s1', 'P6510s2', 'P6545', 'P6545s1', 'P6545s2', 'P6580', 'P6580s1', 'P6580s2', 'P6585s1', 'P6585s1a1', 'P6585s1a2', 'P6585s2', 'P6585s2a1', 'P6585s2a2', 'P6585s3', 'P6585s3a1', 'P6585s3a2', 'P6585s4', 'P6585s4a1', 'P6585s4a2', 'P6590', 'P6590s1', 'P6600', 'P6600s1', 'P6610', 'P6610s1', 'P6620', 'P6620s1', 'P6630s1', 'P6630s1a1', 'P6630s2', 'P6630s2a1', 'P6630s3', 'P6630s3a1', 'P6630s4', 'P6630s4a1', 'P6630s6', 'P6630s6a1', 'P6750', 'P6760', 'P550', 'P6800', 'P6870', 'P6920', 'P7040', 'P7045', 'P7050', 'P7070', 'P7090', 'P7110', 'P7120', 'P7140s1', 'P7140s2', 'P7150', 'P7160', 'P7310', 'P7350', 'P7422', 'P7422s1', 'P7472', 'P7472s1', 'P7495', 'P7500s1', 'P7500s1a1', 'P7500s2', 'P7500s2a1', 'P7500s3', 'P7500s3a1', 'P7505', 'P7510s1', 'P7510s1a1', 'P7510s2', 'P7510s2a1', 'P7510s3', 'P7510s3a1', 'P7510s5', 'P7510s5a1', 'P7510s6', 'P7510s6a1', 'P7510s7', 'P7510s7a1', 'Pet', 'Oc', 'Des', 'Ina', 'Impa', 'Isa', 'Ie', 'Imdi', 'Iof1', 'Iof2', 'Iof3h', 'Iof3i', 'Iof6', 'Cclasnr2', 'Cclasnr3', 'Cclasnr4', 'Cclasnr5', 'Cclasnr6', 'Cclasnr7', 'Cclasnr8', 'Cclasnr11', 'Impaes', 'Isaes', 'Iees', 'Imdies', 'Iof1es', 'Iof2es', 'Iof3hes', 'Iof3ies', 'Iof6es', 'Ingtotob', 'Ingtotes', 'Ingtot', 'Fex_c', 'Depto', 'Fex_dpto']\n"
     ]
    }
   ],
   "source": [
    "print(test_hogares.columns.tolist())\n",
    "print(train_hogares.columns.tolist())\n",
    "print(test_personas.columns.tolist())\n",
    "print(train_personas.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PecLHbWbtFQj"
   },
   "source": [
    "## Fusionar individuos con hogares\n",
    "\n",
    "Se crean las siguientes variables:\n",
    "- Promedio_Educacion: se espera que a mayor nivel educativo del hogar, mayor sera el ingreso\n",
    "\n",
    "- hacinamiento: se supone que a mayor hacinamiento, mas probabilidad tiene el hogar de ser pobre\n",
    "\n",
    "- capacitacion_financiera: se supone que los hogares pobres no tienen capaticacion financiera\n",
    "\n",
    "- jefe_sex: es posible que los hogares con jefas de hogares mujeres tengan menos ingresos que aquellso donde el jefe de hogar es hombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9n1A97VN2O3g"
   },
   "outputs": [],
   "source": [
    "# Crear una nueva variable que sea promedio educativo por hogar considerando individuos mayores de 18 años\n",
    "\n",
    "# Filtrar personas mayores de 18 años en el dataset de entrenamiento\n",
    "train_personas_mayores = train_personas[train_personas['P6040'] > 18]\n",
    "\n",
    "# Agrupar por 'id' y sumar los valores de 'P6210s1' para cada hogar\n",
    "promedio_educacion_train = train_personas_mayores.groupby('id')['P6210s1'].sum().reset_index()\n",
    "promedio_educacion_train.rename(columns={'P6210s1': 'Suma_P6210s1'}, inplace=True)\n",
    "\n",
    "# Unir con el DataFrame de hogares en base al 'id'\n",
    "train_hogares = train_hogares.merge(promedio_educacion_train, on='id', how='left')\n",
    "\n",
    "# Calcular el promedio de educación por hogar\n",
    "train_hogares['Promedio_Educacion'] = train_hogares['Suma_P6210s1'] / train_hogares['Nper']\n",
    "\n",
    "# Repetir el proceso para el conjunto de prueba\n",
    "\n",
    "# Filtrar personas mayores de 18 años en el dataset de prueba\n",
    "test_personas_mayores = test_personas[test_personas['P6040'] > 18]\n",
    "\n",
    "# Agrupar por 'id' y sumar los valores de 'P6210s1' para cada hogar\n",
    "promedio_educacion_test = test_personas_mayores.groupby('id')['P6210s1'].sum().reset_index()\n",
    "promedio_educacion_test.rename(columns={'P6210s1': 'Suma_P6210s1'}, inplace=True)\n",
    "\n",
    "# Unir con el DataFrame de hogares en base al 'id'\n",
    "test_hogares = test_hogares.merge(promedio_educacion_test, on='id', how='left')\n",
    "\n",
    "# Calcular el promedio de educación por hogar\n",
    "test_hogares['Promedio_Educacion'] = test_hogares['Suma_P6210s1'] / test_hogares['Nper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "I1jGJxk5eBKA"
   },
   "outputs": [],
   "source": [
    "# Crear una nueva variable que sea promedio de personas por habitacion\n",
    "\n",
    "train_hogares['hacinamiento'] = train_hogares['Nper'] / train_hogares['P5010']\n",
    "\n",
    "# Calcular el ratio de personas por dormitorio (Aglomeracion) en test_hogares\n",
    "test_hogares['hacinamiento'] = test_hogares['Nper'] / test_hogares['P5010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IssH1ONXgAZ2"
   },
   "outputs": [],
   "source": [
    "# Crear la variable `capacitacion_financiera` en el set de entrenamiento\n",
    "\n",
    "capacitacion_train = train_personas.groupby('id')['P7510s5'].apply(lambda x: 1 if (x == 1).any() else 0).reset_index()\n",
    "capacitacion_train.rename(columns={'P7510s5': 'capacitacion_financiera'}, inplace=True)\n",
    "\n",
    "# Unir con el DataFrame de hogares en base al 'id'\n",
    "train_hogares = train_hogares.merge(capacitacion_train, on='id', how='left')\n",
    "\n",
    "# Crear la variable `capacitacion_financiera` en el set de prueba\n",
    "capacitacion_test = test_personas.groupby('id')['P7510s5'].apply(lambda x: 1 if (x == 1).any() else 0).reset_index()\n",
    "capacitacion_test.rename(columns={'P7510s5': 'capacitacion_financiera'}, inplace=True)\n",
    "\n",
    "# Unir con el DataFrame de hogares en base al 'id'\n",
    "test_hogares = test_hogares.merge(capacitacion_test, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rMJo0kF_byTb"
   },
   "outputs": [],
   "source": [
    "# Crear la variable `jefe_sex` para el conjunto de entrenamiento\n",
    "# Filtrar las personas que son jefes de hogar\n",
    "jefes_train = train_personas[train_personas['P6050'] == 1][['id', 'P6020']]\n",
    "\n",
    "# Renombrar la columna `P6020` a `jefe_sex`\n",
    "jefes_train.rename(columns={'P6020': 'jefe_sex'}, inplace=True)\n",
    "\n",
    "# Unir con el DataFrame de hogares en base al 'id'\n",
    "train_hogares = train_hogares.merge(jefes_train, on='id', how='left')\n",
    "\n",
    "# Crear la variable `jefe_sex` para el conjunto de prueba\n",
    "jefes_test = test_personas[test_personas['P6050'] == 1][['id', 'P6020']]\n",
    "jefes_test.rename(columns={'P6020': 'jefe_sex'}, inplace=True)\n",
    "test_hogares = test_hogares.merge(jefes_test, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww9PaD924VnG"
   },
   "source": [
    "## Estadistica descriptiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "34TX3kzS4L6d"
   },
   "outputs": [],
   "source": [
    "# Estadistica descriptiva de los datos (falta completar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a1cYZKs4iY-"
   },
   "source": [
    "## Modelos\n",
    "1. Regresion lineal por ingreso (Santi) 3\n",
    "2. Logit con 0 y 1 (Luis) 3\n",
    "3. Random Forest (Todos) 3\n",
    "4. Boosting (Gina) 3\n",
    "\n",
    "Trabajar con train_hogares (separando 70/30). test_hogares recien hay que volver a usarlo al final de todo para mandar nuestras predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9JZXJ9vrAav"
   },
   "source": [
    "### 1. Regresion lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Bcx8RxvsrP-e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZhK6gYWrAk_"
   },
   "source": [
    "### 2. Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TgN2L79MrQj9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOJw_r4HrAr-"
   },
   "source": [
    "### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vyRuZtLIrReO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq3Q9VxbrA0W"
   },
   "source": [
    "### 4. Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CK2cMEyBnVV"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los modelos\n",
    "modelo_ada1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
    "modelo_ada2 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100)\n",
    "modelo_ada3 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir la base de datos en entrenamiento y testeo\n",
    "train_df, test_df = train_test_split(train_hogares, test_size=0.3, random_state=123)\n",
    "\n",
    "# Definir las variables predictoras (X) y la variable objetivo (y)\n",
    "X_train = train_df[['hacinamiento', 'capacitacion_financiera', 'P5130', 'P5090', 'jefe_sex']]\n",
    "y_train = train_df['Pobre']\n",
    "X_test = test_df[['hacinamiento', 'capacitacion_financiera', 'P5130', 'P5090', 'jefe_sex']]\n",
    "y_test = test_df['Pobre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar los modelos\n",
    "modelo_ada1.fit(X_train, y_train)\n",
    "modelo_ada2.fit(X_train, y_train)\n",
    "modelo_ada3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular las probabilidades predichas para los tres modelos\n",
    "test_df['prob_hat_ada1'] = modelo_ada1.predict_proba(X_test)[:, 1]\n",
    "test_df['prob_hat_ada2'] = modelo_ada2.predict_proba(X_test)[:, 1]\n",
    "test_df['prob_hat_ada3'] = modelo_ada3.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar las curvas ROC para los tres modelos\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, (modelo, prob_col) in enumerate(zip(\n",
    "        ['Modelo AdaBoost 1', 'Modelo AdaBoost 2', 'Modelo AdaBoost 3'], \n",
    "        ['prob_hat_ada1', 'prob_hat_ada2', 'prob_hat_ada3']\n",
    "    )):\n",
    "    fpr, tpr, _ = roc_curve(y_test, test_df[prob_col])\n",
    "    auc = roc_auc_score(y_test, test_df[prob_col])\n",
    "    plt.plot(fpr, tpr, label=f\"{modelo} (AUC = {auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Línea aleatoria\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curvas ROC')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcionamiento \n",
    "hogares_testeo = test_hogares[['hacinamiento', 'capacitacion_financiera', 'P5130', 'P5090', 'jefe_sex']]\n",
    "\n",
    "# Calcular probabilidades predichas\n",
    "test_hogares['Pobre_est_m1'] = modelo_ada1.predict_proba(hogares_testeo)[:, 1]\n",
    "test_hogares['Pobre_est_m2'] = modelo_ada2.predict_proba(hogares_testeo)[:, 1]\n",
    "test_hogares['Pobre_est_m3'] = modelo_ada3.predict_proba(hogares_testeo)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertir probabilidades a predicciones binarias con un umbral\n",
    "umbral = 0.5 #(usamos la demostracion de clase para justificar el umbral)\n",
    "test_hogares['Pobre_m1_bin'] = (test_hogares['Pobre_est_m1'] >= umbral).astype(int)\n",
    "test_hogares['Pobre_m2_bin'] = (test_hogares['Pobre_est_m2'] >= umbral).astype(int)\n",
    "test_hogares['Pobre_m3_bin'] = (test_hogares['Pobre_est_m3'] >= umbral).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el DataFrame final\n",
    "df_modelo1 = test_hogares[['id', 'Pobre_m1_bin']]\n",
    "df_modelo2 = test_hogares[['id', 'Pobre_m2_bin']]\n",
    "df_modelo3 = test_hogares[['id', 'Pobre_m3_bin']]\n",
    "\n",
    "df_modelo1.to_csv('prediccion modelo 1.csv', index=False, encoding='utf-8')\n",
    "df_modelo2.to_csv('prediccion modelo 2.csv', index=False, encoding='utf-8')\n",
    "df_modelo3.to_csv('prediccion modelo 3.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
